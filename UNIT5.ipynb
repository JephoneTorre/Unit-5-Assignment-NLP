{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f391fb2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wikipedia'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwikipedia\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wikipedia'"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# List of topics you want to fetch from Wikipedia\n",
    "topics = [\"Python (programming language)\", \"Data science\", \"Football\", \"Basketball\", \"Machine learning\"]\n",
    "\n",
    "# Fetch content for each topic from Wikipedia\n",
    "corpus = []\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic)  # Fetch the Wikipedia page for the topic\n",
    "        corpus.append(page.content)  # Get the content of the page\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Disambiguation error for {topic}: {e.options}\")\n",
    "        corpus.append(\"\")  # Append empty string in case of ambiguity\n",
    "    except wikipedia.exceptions.HTTPError:\n",
    "        print(f\"HTTP Error when fetching {topic}.\")\n",
    "        corpus.append(\"\")  # Append empty string in case of error\n",
    "\n",
    "# 1. (20 points) Using Wikipedia as the corpus, obtain 5 different topics that will serve as your documents\n",
    "#    and create a term-document matrix.\n",
    "#    a. Term-document matrix using raw frequency.  \n",
    "vectorizer = CountVectorizer()  # This part corresponds to creating the term-document matrix using raw frequency.\n",
    "raw_frequency_matrix = vectorizer.fit_transform(corpus).toarray()  # Creating the raw frequency matrix\n",
    "raw_frequency_df = pd.DataFrame(raw_frequency_matrix, columns=vectorizer.get_feature_names_out())\n",
    "print(\"Term-Document Matrix (Raw Frequency):\")\n",
    "print(raw_frequency_df)\n",
    "\n",
    "# b. Term-document matrix using TF-IDF weights.\n",
    "tfidf_vectorizer = TfidfVectorizer()  # This part corresponds to creating the term-document matrix using TF-IDF.\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).toarray()  # Creating the TF-IDF matrix\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTerm-Document Matrix (TF-IDF):\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# c. Cosine similarity between documents.\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)  # This part corresponds to calculating the cosine similarity between documents.\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, columns=[f\"Doc{i+1}\" for i in range(len(corpus))], index=[f\"Doc{i+1}\" for i in range(len(corpus))])\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(cosine_sim_df)\n",
    "\n",
    "# d. Find most similar documents.\n",
    "most_similar_docs = np.unravel_index(np.argmax(cosine_sim[np.triu_indices(len(corpus), k=1)]), cosine_sim.shape)  # This part corresponds to finding the most similar documents.\n",
    "print(f\"\\nMost similar documents are Doc{most_similar_docs[0] + 1} and Doc{most_similar_docs[1] + 1}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#THIS ARE THE RESULT THAT I RUN IN A DIFFERENT COMPUTER\n",
    "# Term-Document Matrix (Raw Frequency):\n",
    "#    000  06681  10  100  100x  106  1098155438  11  110  1174  ...  yukihiro \n",
    "# 0    0      0   9    0     1    0           1  10    0     0  ...         1   \n",
    "# 1    0      0   0    0     0    0           0   0    0     0  ...         0   \n",
    "# 2    1      0   1    0     0    1           0   1    0     1  ...         0   \n",
    "# 3    4      1   3    1     0    0           0   1    1     0  ...         0   \n",
    "# 4    0      0   0    0     0    0           0   0    0     0  ...         0   \n",
    "\n",
    "#    zealand  zen  zero  zone  zones  zope  φαινίνδα  ἐπίσκυρος  蹴鞠  \n",
    "# 0        0    2     2     0      0     1         0          0   0  \n",
    "# 1        0    0     0     0      0     0         0          0   0  \n",
    "# 2        8    0     0     0      1     0         1          1   1  \n",
    "# 3        0    0     0     4      0     0         0          0   0  \n",
    "# 4        0    0     0     0      0     0         0          0   0  \n",
    "\n",
    "# [5 rows x 5114 columns]\n",
    "\n",
    "# Term-Document Matrix (TF-IDF):\n",
    "#        000     06681        10       100      100x       106  1098155438  \n",
    "# 0  0.000000  0.000000  0.014472  0.000000  0.002401  0.000000    0.002401   \n",
    "# 1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
    "# 2  0.001192  0.000000  0.000989  0.000000  0.000000  0.001477    0.000000   \n",
    "# 3  0.004824  0.001495  0.003003  0.001495  0.000000  0.000000    0.000000   \n",
    "# 4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.000000   \n",
    "# ...\n",
    "# Doc4  0.637752  0.430567  0.808665  1.000000   0.0\n",
    "# Doc5  0.000000  0.000000  0.000000  0.000000   0.0\n",
    "\n",
    "# Most similar documents are Doc2 and Doc3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
