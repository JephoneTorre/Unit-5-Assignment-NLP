{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f391fb2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wikipedia'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwikipedia\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wikipedia'"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# List of topics you want to fetch from Wikipedia\n",
    "topics = [\"Python (programming language)\", \"Data science\", \"Football\", \"Basketball\", \"Machine learning\"]\n",
    "\n",
    "# Fetch content for each topic from Wikipedia\n",
    "corpus = []\n",
    "for topic in topics:\n",
    "    try:\n",
    "        page = wikipedia.page(topic)  # Fetch the Wikipedia page for the topic\n",
    "        corpus.append(page.content)  # Get the content of the page\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Disambiguation error for {topic}: {e.options}\")\n",
    "        corpus.append(\"\")  # Append empty string in case of ambiguity\n",
    "    except wikipedia.exceptions.HTTPError:\n",
    "        print(f\"HTTP Error when fetching {topic}.\")\n",
    "        corpus.append(\"\")  # Append empty string in case of error\n",
    "\n",
    "# Term-document matrix using raw frequency\n",
    "vectorizer = CountVectorizer()\n",
    "raw_frequency_matrix = vectorizer.fit_transform(corpus).toarray()\n",
    "raw_frequency_df = pd.DataFrame(raw_frequency_matrix, columns=vectorizer.get_feature_names_out())\n",
    "print(\"Term-Document Matrix (Raw Frequency):\")\n",
    "print(raw_frequency_df)\n",
    "\n",
    "# Term-document matrix using TF-IDF weights\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus).toarray()\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"\\nTerm-Document Matrix (TF-IDF):\")\n",
    "print(tfidf_df)\n",
    "\n",
    "# Cosine similarity between documents\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, columns=[f\"Doc{i+1}\" for i in range(len(corpus))], index=[f\"Doc{i+1}\" for i in range(len(corpus))])\n",
    "print(\"\\nCosine Similarity Matrix:\")\n",
    "print(cosine_sim_df)\n",
    "\n",
    "# Find most similar documents\n",
    "most_similar_docs = np.unravel_index(np.argmax(cosine_sim[np.triu_indices(len(corpus), k=1)]), cosine_sim.shape)\n",
    "print(f\"\\nMost similar documents are Doc{most_similar_docs[0] + 1} and Doc{most_similar_docs[1] + 1}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
